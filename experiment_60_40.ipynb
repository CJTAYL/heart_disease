{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60:40 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('mydata.pkl', 'rb') as f:\n",
    "    X_encoded_scaled, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (181, 25) (181,)\n",
      "Testing set size: (122, 25) (122,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded_scaled, y, test_size=.40, random_state=0)\n",
    "\n",
    "# Checking the size of the splits\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "def calculate_acc_ci(accuracy, n, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for a classification accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    - accuracy: The observed accuracy (proportion of correct classifications).\n",
    "    - n: The total number of predictions made (sample size).\n",
    "    - confidence: The desired confidence level.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    # Calculate the z-score for the desired confidence level\n",
    "    z = np.abs(stats.norm.ppf((1 - confidence) / 2))\n",
    "    \n",
    "    # Calculate the margin of error\n",
    "    margin_of_error = z * np.sqrt((accuracy * (1 - accuracy)) / n)\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    ci_lower = accuracy - margin_of_error\n",
    "    ci_upper = accuracy + margin_of_error\n",
    "    \n",
    "    return ci_lower, ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_sensitivity_confidence_interval(TP, FN, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for sensitivity (true positive rate).\n",
    "    \n",
    "    Parameters:\n",
    "    - TP: Number of true positives.\n",
    "    - FN: Number of false negatives.\n",
    "    - confidence_level: Desired confidence level for the interval.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    # Calculate the point estimate of sensitivity\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    \n",
    "    # Calculate the standard error\n",
    "    n = TP + FN\n",
    "    standard_error = sqrt(sensitivity * (1 - sensitivity) / n)\n",
    "    \n",
    "    # Find the z-score for the confidence level\n",
    "    z_score = norm.ppf((1 + confidence_level) / 2)\n",
    "    \n",
    "    # Calculate the margin of error\n",
    "    margin_of_error = z_score * standard_error\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    ci_lower = sensitivity - margin_of_error\n",
    "    ci_upper = sensitivity + margin_of_error\n",
    "    \n",
    "    return ci_lower, ci_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_sensitivity(c_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the true positives, false negatives, and sensitivity from confusion matrix\n",
    "\n",
    "    Parameters:\n",
    "    - TP: Number of true positives.\n",
    "    - FN: Number of false negatives.\n",
    "    - confidence_level: Desired confidence level for the interval.\n",
    "\n",
    "    Returns\n",
    "    - TP: int\n",
    "        Number of true positives in confusion matrix\n",
    "    - FN: int\n",
    "        Number of false negatives in confusion matrix\n",
    "    - sen: float\n",
    "        Sensitivity score     \n",
    "    \"\"\"\n",
    "    TP = c_matrix[1, 1]\n",
    "    FN = c_matrix[1,0]\n",
    "\n",
    "    sen = TP / (TP + FN)\n",
    "\n",
    "    return TP, FN, sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def evaluation_metrics(pred, pred_proba, cm, X_test=X_test, y_test=y_test):\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    _, _, sen = binary_sensitivity(cm)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    auc_roc = roc_auc_score(y_test, pred_proba)\n",
    "    return acc, sen, f1, auc_roc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Acc. Score: 0.81\n",
      "LR Sen. Score: 0.77\n",
      "LR F1 Score: 0.81\n",
      "LR AUC-ROC: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = LogisticRegressionCV(cv=5, random_state=1, solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_scores = lr.predict_proba(X_test)[:, 1]\n",
    "lr_cm = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "lr_acc, lr_sen, lr_f1_score, lr_auc_roc = evaluation_metrics(lr_pred, lr_scores, lr_cm)\n",
    "\n",
    "print(f'LR Acc. Score: {lr_acc:.2f}')\n",
    "print(f'LR Sen. Score: {lr_sen:.2f}')\n",
    "print(f'LR F1 Score: {lr_f1_score:.2f}')\n",
    "print(f'LR AUC-ROC: {lr_auc_roc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'n_estimators': 300}\n",
      "Best cross-validation score: 0.8451951951951951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn. model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100, 200, 300],\n",
    "    'max_depth' : [None, 10, 20, 30],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=2)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLF Acc. Score: 0.76\n",
      "CLF Sen. Score: 0.71\n",
      "CLF F1 Score: 0.75\n",
      "CLF AUC-ROC: 0.86\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, random_state=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf_pred = clf.predict(X_test)\n",
    "clf_scores = clf.predict_proba(X_test)[:, 1]\n",
    "clf_cm = confusion_matrix(y_test, clf_pred)\n",
    "\n",
    "clf_acc, clf_sen, clf_f1_score, clf_auc_roc = evaluation_metrics(clf_pred, clf_scores, clf_cm)\n",
    "\n",
    "print(f'CLF Acc. Score: {clf_acc:.2f}')\n",
    "print(f'CLF Sen. Score: {clf_sen:.2f}')\n",
    "print(f'CLF F1 Score: {clf_f1_score:.2f}')\n",
    "print(f'CLF AUC-ROC: {clf_auc_roc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support-Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 1, 'degree': 2, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Best cross-validation score:  0.8786786786786787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Type of kernel\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'degree': [2, 3, 4]  # Degree of the polynomial kernel function ('poly'). Ignored by other kernels.\n",
    "}\n",
    "\n",
    "# Initialize the SVC\n",
    "svc = SVC(random_state=4)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Acc. Score: 0.80\n",
      "SVM Sen. Score: 0.69\n",
      "SVM F1 Score: 0.78\n",
      "SVM AUC-ROC: 0.88\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', C=1, degree=2, gamma='auto', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_test)\n",
    "svm_scores = svm.predict_proba(X_test)[:, 1]\n",
    "svm_cm = confusion_matrix(y_test, svm_pred)\n",
    "\n",
    "svm_acc, svm_sen, svm_f1_score, svm_auc_roc = evaluation_metrics(svm_pred, svm_scores, svm_cm)\n",
    "\n",
    "print(f'SVM Acc. Score: {svm_acc:.2f}')\n",
    "print(f'SVM Sen. Score: {svm_sen:.2f}')\n",
    "print(f'SVM F1 Score: {svm_f1_score:.2f}')\n",
    "print(f'SVM AUC-ROC: {svm_auc_roc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_neighbors': 5, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.8567567567567569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors' : [3, 5, 7, 9, 11],\n",
    "    'weights' : ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Acc. Score: 0.80\n",
      "KNN Sen. Score: 0.69\n",
      "KNN F1 Score: 0.78\n",
      "KNN AUC-ROC: 0.85\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_scores = knn.predict_proba(X_test)[:, 1]\n",
    "knn_cm = confusion_matrix(y_test, knn_pred)\n",
    "\n",
    "knn_acc, knn_sen, knn_f1_score, knn_auc_roc = evaluation_metrics(knn_pred, knn_scores, knn_cm)\n",
    "\n",
    "print(f'KNN Acc. Score: {knn_acc:.2f}')\n",
    "print(f'KNN Sen. Score: {knn_sen:.2f}')\n",
    "print(f'KNN F1 Score: {knn_f1_score:.2f}')\n",
    "print(f'KNN AUC-ROC: {knn_auc_roc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ACC</th>\n",
       "      <th>SEN</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC-ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.811475</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.887097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.762295</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.861425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.876344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.845565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model       ACC       SEN        F1   AUC-ROC\n",
       "0    LR  0.811475  0.774194  0.806723  0.887097\n",
       "1    RF  0.762295  0.709677  0.752137  0.861425\n",
       "2   SVM  0.803279  0.693548  0.781818  0.876344\n",
       "3   KNN  0.803279  0.693548  0.781818  0.845565"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "models = ['LR', 'RF', 'SVM', 'KNN']\n",
    "acc_scores = [lr_acc, clf_acc, svm_acc, knn_acc]\n",
    "sen_scores = [lr_sen, clf_sen, svm_sen, knn_sen]\n",
    "f1_scores = [lr_f1_score, clf_f1_score, svm_f1_score, knn_f1_score]\n",
    "auc_roc_scores = [lr_auc_roc, clf_auc_roc, svm_auc_roc, knn_auc_roc]\n",
    "\n",
    "results_df = pd.DataFrame(zip(models, acc_scores, sen_scores, f1_scores, auc_roc_scores),\n",
    "                        columns=['Model', 'ACC', 'SEN', 'F1', 'AUC-ROC'])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤═══════╤═══════╤══════╤═══════════╕\n",
      "│ Model   │  ACC  │  SEN  │  F1  │  AUC-ROC  │\n",
      "╞═════════╪═══════╪═══════╪══════╪═══════════╡\n",
      "│ LR      │ 0.81  │ 0.77  │ 0.81 │   0.89    │\n",
      "├─────────┼───────┼───────┼──────┼───────────┤\n",
      "│ RF      │ 0.76  │ 0.71  │ 0.75 │   0.86    │\n",
      "├─────────┼───────┼───────┼──────┼───────────┤\n",
      "│ SVM     │  0.8  │ 0.69  │ 0.78 │   0.88    │\n",
      "├─────────┼───────┼───────┼──────┼───────────┤\n",
      "│ KNN     │  0.8  │ 0.69  │ 0.78 │   0.85    │\n",
      "╘═════════╧═══════╧═══════╧══════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "formatted_results_df = results_df.round(2)\n",
    "\n",
    "print(tabulate(formatted_results_df, headers='keys', tablefmt='fancy_grid', showindex=False, numalign='center'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
